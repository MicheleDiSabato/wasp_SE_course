The problem addressed in this paper (\cite{Vonderhaar}) concerns data augmentation in training ML models, specifically for object detection in images. Object detection involves classifying and locating instances of specific classes in images. For example, given a picture of a cat, the model must recognize the presence of a cat and locate all instances by surrounding them with bounding boxes. For such models to perform well, the training dataset must contain sufficient examples of each class. When this is not the case, a common solution is to artificially augment the dataset with images of under-represented classes. These synthetic images can be generated by suitable generative models. The issues tackled in the paper relate to my research project because performing simulations based on synthetic dataset is often a good way to showcase the capabilities of a mathematical framework aimed at performing hypothesis testing for functional data (as described in the Introduction).\\
What makes a generated image “good” or valid enough to be included in the augmented dataset? Is it possible to check if a procedure consisting in generating images is reliable enough to be used to augmented a dataset which contains under-represented classes? The authors of this paper propose a procedure to check the validity of the augmented images by embedding requirements engineering into the data augmentation procedure. As indicated during the lectures, requirement engineering is a crucial step when developing a software product, especially when the core of the project is a ML model and pipeline. \\
The authors of the paper assume that the task at hand is object detection for images. They assume that the dataset lacks sufficient instances of certain classes. Therefore, the training dataset needs to be \underline{reliably} augmented in a suitable way. Assume that the under-represented class is the class “person”. The augmentation procedure detailed in the paper is as follows: a set of requirements (also called “specifications”) are first formulated, for example: “the object detector shall detect a person if the person is standing close to the image” or “the object detector shall detect a person if the person is sitting down in the bottom-right corner of the image”. Note that these requirements are both requirements for the generated images and requirements for the so-called “downstream model”, i.e. the object detector. Then a prompt is formulated, for example: “generate the picture of a person close-up”. This prompt is fed to a generative model (Flux). The generated image is then manually checked to see if it follows the rules stated in the prompt and the given requirements. Finally, an “auxiliary”, pre-trained object detector (YOLO) is fed the generated image to see if it can spot the person in the image. The idea is that if a pre-trained model works well with the generated synthetic image, then (hopefully) the downstream model (which will be trained with the augmented dataset) will have a chance to be correct as well. If the image passes both the manual check \underline{and} the object-detector actually identifies what it should identify, then the image is included in the augmented dataset.%\par\vspace{0.7em}

The paper’s ideas and my own WASP research can fit together in the following AI-intensive software project. Assume that some stakeholders would like a product that, given brain scans (as images) of children, detects anomalous portions that could be linked with cancer. This means that the ML/AI model is the core of the software product to be developed. Assume that this problem is mathematically framed as described in the Introduction, i.e. as a statistical problem where the goal is to test if each image contains anomalous portions that could be linked with cancer. In this case $H_0$ can be a template image of a cancerous mass. Since, luckily, brain cancer doesn't present itself often in children, the ML model wouldn't have enough images to reach a satisfactory level of accuracy. Reliable data augmentation could be a solution to this problem. In this case, it could be beneficial to adapt the data augmentation procedure described in the paper. This would require the use of an "image segmenter" as downstream model. As a result, the final model would be guaranteed to have been trained on "realistic" images of brain cancer. %\par\vspace{0.7em}

Finally, it could be possible to tweak my research project to support the paper’s AI-engineering idea and the challenges it addresses. The main point of the paper is that it is important to make sure that synthetic data are valid and realistic. The idea proposed by the authors tackles this by integrating requirements engineering in the data generation step. Essentially, each generated image is accepted in the augmented dataset if it is deemed "realistic" enough. This would make the model pipeline more trustworthy to the project's stakeholders. This implementation would require some changes to the methodology provided in the paper, however it is an important point to keep in mind when performing simulations, even in the context of functional data analysis. 
